# Model Details

## ðŸ“š Documentation
- [**System Architecture**](documentation/ARCHITECTURE.md) - High-level overview of the 4-branch model and system design.
- [**Setup Guide**](documentation/SETUP.md) - Instructions for installing and running the project.
- [**Backend API**](documentation/BACKEND_API.md) - API endpoints for the Flask backend.
- [**Frontend & UI**](documentation/FRONTEND.md) - Details about the web interface and UX.
- [**Advanced Usage**](documentation/ADVANCED_USAGE.md) - Fine-tuning, benchmarking, and custom evaluation details.
- [**Glossary**](documentation/GLOSSARY.md) - Definitions of technical terms (CNN, ViT, Diffusion, etc.).

## ðŸŒŸ Key Features
- **Multi-Modal Detection**: Simultaneously analyzes 4 distinct image properties (RGB, Frequency, Patches, Semantics).
- **Explainable AI**: Provides visual **Heatmaps** (Grad-CAM) to show users *why* an image is fake.
- **Easy Deployment**: Includes a full Flask backend and a modern Drag-and-Drop frontend.
- **Transfer Learning Ready**: Scripts to fine-tune the model on your own specific datasets.

## Model Description
**DeepGuard** is a state-of-the-art Multi-Branch Deep Learning system designed to distinguish between real camera-captured images and AI-generated synthetic media (GANs, Diffusion Models). It leverages a hybrid architecture combining Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to analyze images across multiple domains: spatial (RGB), frequency (FFT), local patches, and global semantic consistency.

- **Developed by:** DeepGuard Team
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Model type:** Binary Classifier (Deep Learning: CNN + Transformer Hybrid)
- **Language(s) (NLP):** Python, PyTorch
- **License:** [More Information Needed]
- **Finetuned from model [optional]:** EfficientNetV2-Small, SwinV2-Tiny (Pretrained on ImageNet)

## Model Sources [optional]
- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

# Uses

## Direct Use
The model is designed to detect deepfake images generated by modern AI models (like GANs and Diffusion models). It takes an image as input and outputs a probability score (Real vs. Fake) along with a Grad-CAM heatmap highlighting suspicious regions.

## Downstream Use [optional]
[More Information Needed]

## Out-of-Scope Use
[More Information Needed]

## Bias, Risks, and Limitations
[More Information Needed]

## Recommendations
Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.

# How to Get Started with the Model

Use the code below to get started with the model.

```python
import torch
from src.models import DeepfakeDetector
from src.inference import predict_image

# 1. Initialize the model
device = "cuda" if torch.cuda.is_available() else "cpu"
model = DeepfakeDetector(pretrained=True).to(device)

# 2. Load Weights (Ensure you have the checkpoint)
checkpoint_path = "results/checkpoints/best_model.safetensors"
if os.path.exists(checkpoint_path):
    from safetensors.torch import load_file
    model.load_state_dict(load_file(checkpoint_path))
    model.eval()

# 3. Run Inference
image_path = "test_images/sample.jpg"
result = predict_image(model, image_path, device)

print(f"Prediction: {result['label']}")
print(f"Confidence: {result['confidence']:.2f}%")
```

# Training Details

## Training Data
The model is trained on a dataset containing labeled "Real" and "Fake" images.
- **Source:** User-provided dataset located at `data` directory.
- **Structure:** The dataset is expected to have subfolders where image source (Real/Fake) is inferred from the folder name.
- **Split:** Automatic 80/20 train/validation split if single directory provided.

## Training Procedure

### Preprocessing [optional]
- **Resize:** Images are resized to 256x256 pixels.
- **Normalization:** Standard ImageNet normalization (Mean: [0.485, 0.456, 0.406], Std: [0.229, 0.224, 0.225]).
- **Augmentations (Train):**
    - Horizontal Flip (p=0.5)
    - Random Brightness Contrast (p=0.2)
    - Gauss Noise (p=0.2)
    - Image Compression (Quality 60-100, p=0.3)

### Training Hyperparameters
- **Training regime:** Mixed Precision (FP16) or Standard FP32 depending on hardware.
- **Batch Size:** 32
- **Epochs:** 3
- **Optimizer:** AdamW
- **Learning Rate:** 1e-4
- **Weight Decay:** 1e-5
- **LR Scheduler:** StepLR (Step size: 5, Gamma: 0.5)
- **Loss Function:** BCEWithLogitsLoss

## Speeds, Sizes, Times [optional]
[More Information Needed]

# Evaluation

## Testing Data, Factors & Metrics

### Testing Data
[More Information Needed]

### Factors
[More Information Needed]

### Metrics
- **Primary Metric:** Accuracy (Binary Classification)
- **Loss:** Binary Cross Entropy

## Results
[More Information Needed]

# Summary

## Model Examination [optional]
[More Information Needed]

# Environmental Impact

Carbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).

- **Hardware Type:** [More Information Needed] (e.g., Apple M-Series / NVIDIA GPU)
- **Hours used:** [More Information Needed]
- **Cloud Provider:** [More Information Needed]
- **Compute Region:** [More Information Needed]
- **Carbon Emitted:** [More Information Needed]

# Technical Specifications [optional]

## Model Architecture and Objective
The architecture consists of four parallel branches that extract different types of features:
1.  **RGB Branch**: Uses **EfficientNetV2-Small** to capture high-level spatial features and visual artifacts.
2.  **Frequency Branch**: Applies FFT (Fast Fourier Transform) to the image and processes the frequency spectrum using a custom CNN to detect grid-like artifacts common in GANs.
3.  **Patch Branch**: Splits the image into local patches and uses a shared encoder to detect local inconsistencies.
4.  **ViT Branch**: Uses **Swin Transformer V2 Tiny** to model long-range global dependencies and semantic logic.

The features from all branches are concatenated and passed through a final classification head (Linear -> ReLU -> Dropout -> Linear).

## Compute Infrastructure
[More Information Needed]

### Hardware
[More Information Needed]

### Software
- **Framework:** PyTorch
- **Libraries:** Torchvision, Albumentations, NumPy, OpenCV, SafeTensors.

# Citation [optional]

## BibTeX:
[More Information Needed]

## APA:
[More Information Needed]

# Glossary [optional]
[More Information Needed]

# More Information [optional]
[More Information Needed]

# Model Card Authors [optional]
[More Information Needed]

# Model Card Contact
[More Information Needed]
